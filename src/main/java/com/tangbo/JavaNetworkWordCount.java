//package com.tangbo;
//
//import org.apache.spark.SparkConf;
//import org.apache.spark.api.java.JavaRDD;
//import org.apache.spark.api.java.Optional;
//import org.apache.spark.api.java.StorageLevels;
//import org.apache.spark.api.java.function.FlatMapFunction;
//import org.apache.spark.api.java.function.Function2;
//import org.apache.spark.api.java.function.MapFunction;
//import org.apache.spark.api.java.function.PairFunction;
//import org.apache.spark.sql.Dataset;
//import org.apache.spark.sql.Encoders;
//import org.apache.spark.sql.Row;
//import org.apache.spark.sql.SparkSession;
//import org.apache.spark.streaming.Durations;
//import org.apache.spark.streaming.StreamingContext;
//import org.apache.spark.streaming.api.java.JavaDStream;
//import org.apache.spark.streaming.api.java.JavaPairDStream;
//import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
//import org.apache.spark.streaming.api.java.JavaStreamingContext;
//import scala.Tuple2;
//
//import java.util.Arrays;
//import java.util.Iterator;
//import java.util.regex.Pattern;
//
///**
// * Created by tangbo on 16/9/7.
// */
//public class JavaNetworkWordCount {
//
//	private static final Pattern SPACE = Pattern.compile(" ");
//
//	public static void main(String[] args) throws Exception {
//		/*if (args.length < 2) {
//			System.err.println("Usage: JavaNetworkWordCount <hostname> <port>");
//			System.exit(1);
//		}*/
//
//		// Create the context with a 1 second batch size
//		SparkConf sparkConf = new SparkConf().setAppName("JavaNetworkWordCount").setMaster("local[2]");
//		JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, Durations.seconds(5));
//		ssc.checkpoint(".");
//		// Create a JavaReceiverInputDStream on target ip:port and count the
//		// words in input stream of \n delimited text (eg. generated by 'nc')
//		// Note that no duplication in storage level only for running locally.
//		// Replication necessary in distributed scenario for fault tolerance.
//		JavaReceiverInputDStream<String> lines = ssc.socketTextStream(
//				"localhost", 9999, StorageLevels.MEMORY_AND_DISK_SER);
//		JavaDStream<String> words = lines.flatMap(new FlatMapFunction<String, String>() {
//			@Override
//			public Iterator<String> call(String x) {
//				return Arrays.asList(SPACE.split(x)).iterator();
//			}
//		});
//
//		words.foreachRDD((rdd, time) -> {
//			SparkSession spark = SparkSession.builder().config(rdd.context().getConf()).getOrCreate();
//
//			JavaRDD<JavaRow> rowRDD = rdd.map(word -> {
//				JavaRow row = new JavaRow();
//				row.setWord(word);
//				return row;
//			});
//
//			Dataset wordsDataSet = spark.createDataFrame(rowRDD, JavaRow.class);
//			wordsDataSet.createOrReplaceTempView("words");
//			Dataset<Row> wordCountsDataFrame = spark.sql("select word, count(*) as total from words group by word");
////			wordCountsDataFrame.map((MapFunction<Row, String>) row -> {
////				row.getString(0);
////				return null;
////			}, Encoders.STRING());
////			wordCountsDataFrame.show();
//		});
//
//		JavaPairDStream<String, Integer> wordCounts = words.mapToPair(
//				new PairFunction<String, String, Integer>() {
//					@Override
//					public Tuple2<String, Integer> call(String s) {
//						return new Tuple2<>(s, 1);
//					}
//				}).reduceByKeyAndWindow((v1, v2) -> v1 + v2, Durations.seconds(30), Durations.seconds(10)).updateStateByKey((v1, v2) -> {
//					v1.forEach(System.out::print);
//				System.out.println(v2);
//		  	return Optional.of(1);
//		});/*.reduceByKey(new Function2<Integer, Integer, Integer>() {
//			@Override
//			public Integer call(Integer i1, Integer i2) {
//				return i1 + i2;
//			}
//		});*/
//
//
//
//		wordCounts.print();
//		ssc.start();
//		ssc.awaitTermination();
//	}
//
//}
